{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, math\n",
    "import matplotlib.pyplot as plt\n",
    "root = os.path.abspath(os.getcwd())\n",
    "libs = root + '/libs'\n",
    "data = root + '/data'\n",
    "cobras = root + '/cobras_ts'\n",
    "sys.path.append(libs)\n",
    "sys.path.append(data)\n",
    "sys.path.append(cobras)\n",
    "from nn_utils import *\n",
    "from utilsData import *\n",
    "from utilsData import read_dataset\n",
    "from utilsData import transform_labels\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import ctypes\n",
    "import svn\n",
    "import svn.remote\n",
    "import svn.local\n",
    "import sklearn \n",
    "%matplotlib inline\n",
    "from sqlalchemy import create_engine\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "from IPython import display\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from cobras_dtw import COBRAS_DTW\n",
    "from notebookquerier_ts import NotebookQuerierTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UNSIGN_TO_SIGN_SHIFT = 128\n",
    "# Define data\n",
    "listx_test            = []\n",
    "listx_train           = []\n",
    "listy_test            = []\n",
    "listy_train           = []\n",
    "listAdaptedy_test     = []\n",
    "listAdaptedy_train    = []\n",
    "defaultY              = [1,0,0,0]\n",
    "adaptedY              = [1,0,0,0,0]\n",
    "\n",
    "dataRoot              = \"data\"\n",
    "dataset_name          = 'Cloth'\n",
    "rootWithSlash         = dataRoot + '/' \n",
    "dataTest              = dataset_name + \"_TEST.txt\"\n",
    "dataTrain             = dataset_name + \"_TRAIN.txt\"\n",
    "\n",
    "svnRepoMagChannel     = \"http://svn.hamilton.ch/svn/fw_XRP/branches/XMC4xxx/MagChannel/XRP_WM_T_2_0X_00_0007/Source\"\n",
    "svnRepoCheckoutDest   = \"D:/tempTrunk/\"\n",
    "\n",
    "pickleObjectNameW      = 'lowstandardhighvolumetipswater.pkl'\n",
    "pickleObjectNameG      = 'lowstandardhighvolumetipsglycerin.pkl'\n",
    "\n",
    "weightsMagChannel     = \"../../../Source/NN/inc\"\n",
    "weightsMagChannelAbs  = os.path.abspath(weightsMagChannel) + '/'\n",
    "\n",
    "INTERPOLATION         = 4\n",
    "NUMBER_OF_SAMPLES_CNN = 250\n",
    "ADM_LOWER_LIMIT_INT_8 = 127\n",
    "ADM_UPPER_LIMIT_INT_8 = ADM_LOWER_LIMIT_INT_8\n",
    "\n",
    "budget                = 100\n",
    "alpha                 = 0.5\n",
    "window                = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Acquired Aspiration Data From SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_USER              = 'root'\n",
    "SQL_PASSWORT          = 'C00lman.9'\n",
    "localhost             = 'localhost'\n",
    "SQL_PORT              = '3306'\n",
    "databaseName          = 'phänomendetection'\n",
    "datasheetName         = 'asp_cloth_air_maggain20highvolumetip'\n",
    "\n",
    "db_connection_str     = 'mysql+pymysql://'+ SQL_USER + ':' + SQL_PASSWORT+'@' + localhost +':' + SQL_PORT + '/' + databaseName\n",
    "\n",
    "#sql_DF                = pd.read_sql_table(datasheetName, con=db_connection_str)\n",
    "sql_DF                = pd.read_pickle(pickleObjectNameW)\n",
    "#sql_DF                = sql_DF.drop([52])\n",
    "#sql_DF                = sql_DF.drop([69])\n",
    "#sql_DF                = sql_DF.drop([115])\n",
    "\n",
    "#SQL_USER              = 'root'\n",
    "#SQL_PASSWORT          = 'C00lman.9'\n",
    "#localhost             = 'localhost'\n",
    "#SQL_PORT              = '3306'\n",
    "#databaseName          = 'phänomendetection'\n",
    "#datasheetName         = 'asp_cloth_air_maggain20'\n",
    "#\n",
    "#db_connection_str     = 'mysql+pymysql://'+ SQL_USER + ':' + SQL_PASSWORT+'@' + localhost +':' + SQL_PORT + '/' + databaseName\n",
    "#\n",
    "#sql_DF_2              = pd.read_sql_table(datasheetName, con=db_connection_str)\n",
    "sql_DF_2               = pd.read_pickle(pickleObjectNameG)\n",
    "#sql_DF_2              = sql_DF_2.drop([19])\n",
    "#sql_DF_2              = sql_DF_2.drop([60])\n",
    "#sql_DF_2              = sql_DF_2.drop([116])\n",
    "#\n",
    "#sql_DF                = sql_DF.append(sql_DF_2)\n",
    "sql_DF                = pd.concat([sql_DF_2, sql_DF])\n",
    "\n",
    "# Select just pressure datas \n",
    "sql_DFFilteredAsp        = sql_DF.loc[sql_DF.loc[:,'J_Read Pressure Data Active'] == 1,:]\n",
    "# Select just class ids and pressure datas \n",
    "sql_DFFilteredAspColumns = sql_DFFilteredAsp.iloc[:,20:]\n",
    "sql_DFFilteredAspColumns['ClassId'] = sql_DFFilteredAsp.loc[:,'ClassId']\n",
    "sql_DFFilteredAspColumns = sql_DFFilteredAspColumns.reset_index()\n",
    "sql_DFFilteredAspColumns = sql_DFFilteredAspColumns.drop(['index'], axis=1)\n",
    "#sql_DFFilteredAspColumns = sql_DFFilteredAspColumns.drop([288]) \n",
    "#sql_DFFilteredAspColumns = sql_DFFilteredAspColumns.drop([290]) \n",
    "#sql_DFFilteredAspColumns = sql_DFFilteredAspColumns.drop([258]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Aspiration Curves Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = go.Figure()\n",
    "\n",
    "title = 'Different Aspiration Curves Raw Data'\n",
    "# Add traces\n",
    "\n",
    "for i in range(0, sql_DFFilteredAspColumns.shape[0]):\n",
    "    # Normal Aspiration\n",
    "    if int(sql_DFFilteredAspColumns.loc[i,'ClassId']) == 0:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(np.array(sql_DFFilteredAspColumns.iloc[i,20:]))), y=np.array(sql_DFFilteredAspColumns.iloc[i,20:]),name='Asp. Normal', \n",
    "                                 line=dict(color='rgb(0,0,0)', width=1)))\n",
    "    # Air Aspiration\n",
    "    elif int(sql_DFFilteredAspColumns.loc[i,'ClassId']) == 1:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(np.array(sql_DFFilteredAspColumns.iloc[i,20:]))), y=np.array(sql_DFFilteredAspColumns.iloc[i,20:]),name='Asp. Air', \n",
    "                                 line=dict(color='rgb(255,0,0)', width=1)))       \n",
    "    # Cloth Aspiration\n",
    "    elif int(sql_DFFilteredAspColumns.loc[i,'ClassId']) == 2:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(np.array(sql_DFFilteredAspColumns.iloc[i,20:]))), y=np.array(sql_DFFilteredAspColumns.iloc[i,20:]),name='Asp. Cloth', \n",
    "                                 line=dict(color='rgb(0,0,255)', width=1)))  \n",
    "        \n",
    "        \n",
    "annotations = []        \n",
    "        \n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text=title,\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))        \n",
    "\n",
    "fig.update_layout(annotations=annotations,\n",
    "                   yaxis_title='Pressure [pa]',\n",
    "                   xaxis_title='Number of Samples [N] ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Time Data to 250 Points\n",
    "\n",
    "Two different pressure data cases are possible\n",
    " - Aspiration pressure curve contains less than 250 points -> upsampling case\n",
    " - Aspiration pressure curve contains more than 250 points -> Downsampling case\n",
    " \n",
    "The idea for this problem is to transform a digital signal to a nearly continuous signal by interpolation and resample it such that the whole curve in time contains 250 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Interpolation of Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_DFFilteredAspColumnsCopy                = sql_DFFilteredAspColumns.copy()\n",
    "sql_DFFilteredAspColumnsCopy                = sql_DFFilteredAspColumnsCopy.drop(['ClassId'], axis= 1)\n",
    "newDataframe                                = sql_DFFilteredAspColumnsCopy\n",
    "for interpol in range(0,INTERPOLATION):\n",
    "    newDataframe                            = pd.DataFrame()\n",
    "    for i in range(0, sql_DFFilteredAspColumns.shape[0]):\n",
    "        sql_DFFilteredAspColumnsWithoutNan  = np.array(sql_DFFilteredAspColumnsCopy.iloc[i,:].dropna())\n",
    "        dataNumberingArray                  = np.arange(len(sql_DFFilteredAspColumnsWithoutNan)) + 1\n",
    "        zeroPaddedData                      = np.insert(sql_DFFilteredAspColumnsWithoutNan, dataNumberingArray, 0)\n",
    "        x = 0\n",
    "        while x < (len(zeroPaddedData) - 2):\n",
    "            zeroPaddedData[x + 1]           = ((zeroPaddedData[x] - zeroPaddedData[x + 2]) / 2.0) + zeroPaddedData[x + 2]\n",
    "            x = x + 2\n",
    "    \n",
    "        interpolatedData                    = pd.DataFrame(zeroPaddedData)\n",
    "        interpolatedDataT                   = interpolatedData.T\n",
    "        \n",
    "        newDataframe                        = pd.concat([newDataframe, interpolatedDataT])\n",
    "        \n",
    "    newDataframe                            = newDataframe.reset_index()\n",
    "    newDataframe                            = newDataframe.drop(['index'], axis=1)\n",
    "                                                    \n",
    "    sql_DFFilteredAspColumnsCopy            = newDataframe      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Curves Such That Wohle Curve Is Sampled Equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalResampledData                          = pd.DataFrame()\n",
    "for i in range(0, sql_DFFilteredAspColumns.shape[0]):\n",
    "    sampleLengthInterpolatedCurve           = len(sql_DFFilteredAspColumnsCopy.iloc[i, :].dropna())\n",
    "    numberOfOffsetSamples                   = sampleLengthInterpolatedCurve/NUMBER_OF_SAMPLES_CNN\n",
    "    sampleIteration                         = 0\n",
    "    newDataframeWithoutNan                  = sql_DFFilteredAspColumnsCopy.iloc[i, :].dropna()\n",
    "    resampledData                           = []\n",
    "    for k in range(0, NUMBER_OF_SAMPLES_CNN):\n",
    "        resampledData.append(newDataframeWithoutNan[int(sampleIteration)])\n",
    "        sampleIteration                    +=numberOfOffsetSamples\n",
    "    \n",
    "    resampledDataPd                         = pd.DataFrame(np.array(resampledData))\n",
    "    totalResampledData                      = pd.concat([totalResampledData, resampledDataPd.T])\n",
    "  \n",
    "totalResampledData                          = totalResampledData.reset_index()\n",
    "totalResampledData                          = totalResampledData.drop(['index'], axis=1)\n",
    "sql_DFFilteredAspColumnsResIndex            = sql_DFFilteredAspColumns.reset_index()\n",
    "sql_DFFilteredAspColumnsResIndex            = sql_DFFilteredAspColumnsResIndex.drop(['index'], axis=1)\n",
    "totalResampledData['ClassId']               = list(sql_DFFilteredAspColumnsResIndex.loc[:,'ClassId'])\n",
    "# Make sure ClassId stands at first position\n",
    "totalResampledDataColumns                   = totalResampledData.columns.tolist()\n",
    "totalResampledDataColumns                   = totalResampledDataColumns[-1:] + totalResampledDataColumns[:-1]\n",
    "totalResampledData                          = totalResampledData[totalResampledDataColumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalResampledData.to_csv(rootWithSlash+dataTest, index=False, header=False)\n",
    "totalResampledData.to_csv(rootWithSlash+dataTrain, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cobras TS Automatic Clustering Algorithm As Data Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.loadtxt(rootWithSlash+dataTest, delimiter=',',skiprows=0)\n",
    "#series = data[:,1:] # you should only do this if your first column are class labels, as they are in the UCR files\n",
    "#series = series[:100,:] # taking only the first 10 series for example purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constructing the pairwise affinity matrix\n",
    "#dists = dtw.distance_matrix(series, window=int(0.01 * window * series.shape[1]))\n",
    "#dists[dists == np.inf] = 0\n",
    "#dists = dists + dists.T - np.diag(np.diag(dists))\n",
    "#affinities = np.exp(-dists * alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting the clustering process\n",
    "### Queried pairs will be plotted in the notebook\n",
    "### A must-link is given by entering 'Y' in the notebook prompt, a cannot-link by entering 'n'.\n",
    "### Each time an intermediate clustering is shown, you can decide to stop clustering (by entering 'n' in the prompt),\\\n",
    "### or continue answering queries (by entering  'Y' in the prompt).\n",
    "#clusterer = COBRAS_DTW(affinities, NotebookQuerierTS(data), budget)\n",
    "#clustering, intermediate_clusterings, runtimes, ml, cl = clusterer.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After clustering, you can ask for the cluster labeling.\n",
    "#print(clustering.construct_cluster_labeling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return new classification back to \n",
    "#data[0:len(clustering.construct_cluster_labeling()), 0] = clustering.construct_cluster_labeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(rootWithSlash+dataTest ,data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Test/Train Cap Level Json File to Dataframe and Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load text file\n",
    "\n",
    "datasets_dict = read_dataset(dataRoot, dataset_name) \n",
    "x_train       = datasets_dict[dataset_name][0]\n",
    "y_train       = datasets_dict[dataset_name][1]\n",
    "x_test        = datasets_dict[dataset_name][2]\n",
    "y_test        = datasets_dict[dataset_name][3]\n",
    "y_trainClass  = datasets_dict[dataset_name][1]\n",
    "y_testClass   = datasets_dict[dataset_name][3]\n",
    "\n",
    "nb_classes = len(np.unique(np.concatenate((y_train,y_test),axis =0)))\n",
    "# make the min to zero of labels\n",
    "y_train,y_test = transform_labels(y_train,y_test)\n",
    "\n",
    "# save orignal y because later we will use binary\n",
    "y_true = y_test.astype(np.int64) \n",
    "# transform the labels from integers to one hot vectors\n",
    "enc = sklearn.preprocessing.OneHotEncoder()\n",
    "enc.fit(np.concatenate((y_train,y_test),axis =0).reshape(-1,1))\n",
    "y_train = enc.transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.transform(y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "if len(x_train.shape) == 2: # if univariate \n",
    "    # add a dimension to make it multivariate with one dimension \n",
    "    x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],1))\n",
    "    x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Aspiration Data\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta_s = max(S(t)) - min(S(t)) \\\\\n",
    "S_s(t) = \\frac{\\Delta_{s128}}{\\Delta_s} \\Bigg(S(t) - \\bigg|\\frac{|max(S(t))| - |min(S(t))|}{2}\\bigg|\\Bigg) \\hspace{1cm} |max(S(t))| > |min(S(t))|\\\\\n",
    "S_s(t) = \\frac{\\Delta_{s128}}{\\Delta_s} \\Bigg(S(t) + \\bigg|\\frac{|max(S(t))| - |min(S(t))|}{2}\\bigg|\\Bigg) \\hspace{1cm} |min(S(t))| > |max(S(t))|\n",
    "\\end{align}\n",
    "\n",
    "where $S_s(t)$ defines the scaled pressure data, $S(t)$ defines the unscaled pressure data, $\\Delta_{s128}$ the max range of a int_8 datatype (255) and $\\Delta_s$ the max range of the unscaled pressure data $S(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = go.Figure()\n",
    "\n",
    "title = 'Different Aspiration Curves Test Data'\n",
    "# Add traces\n",
    "\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    # Normal Aspiration\n",
    "    if int(y_testClass[i]) == 0:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=x_test[i,:].ravel(),name='Asp. Normal', \n",
    "                                 line=dict(color='rgb(0,0,0)', width=1)))\n",
    "    # Air Aspiration\n",
    "    elif int(y_testClass[i]) == 1:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=x_test[i,:].ravel(),name='Asp. Air', \n",
    "                                 line=dict(color='rgb(255,0,0)', width=1)))       \n",
    "    # Cloth Aspiration\n",
    "    elif int(y_testClass[i]) == 2:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=x_test[i,:].ravel(),name='Asp. Cloth', \n",
    "                                 line=dict(color='rgb(0,0,255)', width=1)))  \n",
    "        \n",
    "        \n",
    "annotations = []        \n",
    "        \n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text=title,\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))        \n",
    "\n",
    "fig.update_layout(annotations=annotations,\n",
    "                   yaxis_title='Pressure [pa]',\n",
    "                   xaxis_title='Number of Samples [N] ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Train X (time) and Y (Normal Aspiration, Cloth Aspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "title = 'Different Aspiration Curves Train Data'\n",
    "scaledTrain = x_train\n",
    "# Add traces\n",
    "for i in range(0,x_train.shape[0]):\n",
    "    # Scale data\n",
    "    temp        = []\n",
    "    data        = x_train[i]\n",
    "    delta_s     = max(data) - min(data)\n",
    "    if(abs(max(data)) > abs(min(data))):\n",
    "        tempScaled          = 253.0/delta_s * np.array(data) \n",
    "        scaledTrain[i,:]    = tempScaled + (ADM_UPPER_LIMIT_INT_8 - max(tempScaled))\n",
    "    elif(abs(min(data)) > abs(max(data))): \n",
    "        tempScaled          = 253.0/delta_s * np.array(data) \n",
    "        scaledTrain[i,:]    = tempScaled + (-ADM_LOWER_LIMIT_INT_8 - min(tempScaled))\n",
    "    \n",
    "    # Normal Aspiration\n",
    "    if int(y_trainClass[i]) == 0:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_train[i])), y=scaledTrain[i,:].ravel(),name='Asp. Normal', \n",
    "                                 line=dict(color='rgb(0,0,0)', width=1)))\n",
    "    # Air Aspiration Aspiration\n",
    "    elif int(y_trainClass[i]) == 1:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_train[i])), y=scaledTrain[i,:].ravel(),name='Asp. Air', \n",
    "                                 line=dict(color='rgb(255,0,0)', width=1)))       \n",
    "    # Cloth Aspiration\n",
    "    elif int(y_trainClass[i]) == 2 :\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_train[i])), y=scaledTrain[i,:].ravel(),name='Asp. Cloth', \n",
    "                                 line=dict(color='rgb(0,0,255)', width=1)))  \n",
    "        \n",
    "annotations = []        \n",
    "        \n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.0,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text=title,\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))        \n",
    "\n",
    "fig.update_layout(annotations=annotations,\n",
    "                   yaxis_title='Pressure [pa]',\n",
    "                   xaxis_title='Number of Samples [N] ')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Test X (time) and Y (Normal Aspiration, Cloth Aspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = go.Figure()\n",
    "\n",
    "title = 'Different Aspiration Curves Test Data'\n",
    "scaledTest = x_test\n",
    "# Add traces\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    # Scale data\n",
    "    temp        = []\n",
    "    data        = x_test[i]\n",
    "    delta_s     = max(data) - min(data)\n",
    "    if(abs(max(data)) > abs(min(data))):\n",
    "        tempScaled          = 253.0/delta_s * np.array(data) \n",
    "        scaledTest[i,:]    = tempScaled + (ADM_UPPER_LIMIT_INT_8 - max(tempScaled))\n",
    "    elif(abs(min(data)) > abs(max(data))): \n",
    "        tempScaled          = 253.0/delta_s * np.array(data) \n",
    "        scaledTest[i,:]    = tempScaled + (-ADM_LOWER_LIMIT_INT_8 - min(tempScaled))\n",
    "\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    # Normal Aspiration\n",
    "    if int(y_testClass[i]) == 0:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=scaledTest[i,:].ravel(),name='Asp. Normal', \n",
    "                                 line=dict(color='rgb(0,0,0)', width=1)))\n",
    "    # Air Aspiration\n",
    "    elif int(y_testClass[i]) == 1:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=scaledTest[i,:].ravel(),name='Asp. Air', \n",
    "                                 line=dict(color='rgb(255,0,0)', width=1)))       \n",
    "    # Cloth Aspiration\n",
    "    elif int(y_testClass[i]) == 2:\n",
    "        fig.add_trace(go.Scatter(x=np.arange(len(x_test[i])), y=scaledTest[i,:].ravel(),name='Asp. Cloth', \n",
    "                                 line=dict(color='rgb(0,0,255)', width=1)))  \n",
    "        \n",
    "        \n",
    "annotations = []        \n",
    "        \n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text=title,\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))        \n",
    "\n",
    "fig.update_layout(annotations=annotations,\n",
    "                   yaxis_title='Pressure [pa]',\n",
    "                   xaxis_title='Number of Samples [N] ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layers of CNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/\n",
    "def KModel(x_train, y_train, x_test, y_test):\n",
    "    # Shuffle\n",
    "    permutation = np.random.permutation(y_train.shape[0])\n",
    "    x_train = x_train[permutation, :, :]\n",
    "    y_train = y_train[permutation]\n",
    "       \n",
    "    inputs = Input(shape=x_train.shape[1:])\n",
    "    # Layer Block 1\n",
    "    x = Conv1D(32, kernel_size=(9), strides=(2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool1D(2, strides=2)(x)\n",
    "\n",
    "    # Layer Block 2\n",
    "    x1 = Conv1D(32, kernel_size=(5), strides=(1), padding=\"same\")(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    x1 = ReLU()(x1)\n",
    "    x1 = MaxPool1D(2, strides=2)(x1)\n",
    "\n",
    "    # Layer Block 3\n",
    "    x2 = Conv1D(32, kernel_size=(3), strides=(1), padding=\"same\")(x)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    x2 = ReLU()(x2)\n",
    "    x2 = MaxPool1D(2, strides=2)(x2)\n",
    "\n",
    "    # Layer Block 4\n",
    "    x3 = MaxPool1D(2, strides=2)(x)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "\n",
    "    # concate all inception layers\n",
    "    x = concatenate([ x1, x2,x3], axis=-1)\n",
    "\n",
    "    # conclusion\n",
    "    x = Conv1D(48, kernel_size=(3), strides=(1), padding=\"same\")(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool1D(2, strides=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # our netowrk is not that deep, so a hidden fully connected layer is introduce\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(3)(x)\n",
    "    y = Softmax()(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, batch_size=16, epochs=100, shuffle=True, verbose=2, validation_data=(x_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists('.shift_list'):\n",
    "    os.remove(\".shift_list\")\n",
    "model = KModel(scaledTrain, y_train, scaledTest, y_test)\n",
    "model.save(dataRoot+'model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout SVN Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SVN Client\n",
    "#client = svn.remote.RemoteClient(svnRepoMagChannel)\n",
    "#client.checkout(svnRepoCheckoutDest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate C Code for NN Library and Transfer Weights File SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights= svnRepoCheckoutDest + 'NN_weights.h'\n",
    "weights= 'D:/ProjectClotDetection/MagChannelImplementation/branches/XRP_WM_T_2_0X_00_0008/Source/NN/' + 'NN_weights.h'\n",
    "generate_model(model, x_test, name=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit SVN Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Committing the actuall changes\n",
    "#r = svn.local.LocalClient(svnRepoCheckoutDest)\n",
    "#r.commit('Updated weights of Mag Channel Phenomen Detection CNN algorithm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
